---
title: "Resilience Model Uncertainty Analysis"
author: "Roy Emanuel II"
output:
    html_document
    # md_document:
        #variant: markdown_github

---
# Resilience Models
## Quotient resilience and total quotient resilience
## Expected System Degradation Function (ESDF) and resilience factor
## Integral resilience
For simplicity we will start out with just the extended integral
resilience metric becaus I think this is the one that matters. We have
shown with the simple stories that they are not viable for these
situations. Trying to rebuild the situation from the
anyLogicDataPull.R file. Small change
```{r}
uncScenFast <- function(data, simRun, need, resFactors, timeHorizon=NULL){
    resMat <- data.frame()
    needStep <- dim(need)[1]
    resStep <- dim(resFactors)[1]

}
```
# Uncertainty Communication

The focus of this section is to build the format for working through
the analysis of the uncertainty quantification of the different 
resilience metrics.
## Time to failure
The systems will typically be exposed to failure multiple times over
the time horizon. Consider this some sort of renewal process. This is
the important thing to consider here. 
### HPP
Simplest is the Homogeneous Poisson process (HPP). Actually, no, what
I want to do is use the exponential distribution, and sample from it
until the time runs out. 

scenarioBuild builds a data frame that produces failure and recovery
events that accumulate until the timehorizon is surpassed. These
are used by performance build to build a performance profile that
can fit into the resilience metric calculators


performanceBuild takes the built scenarios and makes time-sereis
data that is ingested by the resilience calculators

```{r Performance Profile Building}
library("plyr")
library("dplyr")
library("caTools")
set.seed(8541984)
scenarioBuild <- function(lambda, timeHorizon, numScen){
    outDF <- data.frame()
    for(s in 1:numScen){
        DF <- data.frame()
        t <- 0
        while(t < timeHorizon){
            f <- ceiling(rexp(n = 1, lambda))
            fLev <- rbeta(n = 1, shape1 = 1, shape2 = 5)
            fDF <- data.frame(time = f,
                              event = "Fail",
                              scenario = s,
                              Level = fLev)
            DF <- bind_rows(DF, fDF)
            t <- t + f
            if(t > timeHorizon){break}
            r <- ceiling(rexp(n = 1, lambda))
            rLev <- rbeta(n = 1, shape1 = 5, shape2 = 1)
            rDF <- data.frame(time = r,
                              event = "Recover",
                              scenario = s,
                              Level = rLev)
            DF <- bind_rows(DF, rDF)
            t <- t + r
        }
        DF$cumFail <- cumsum(DF$time)
        outDF <- bind_rows(outDF, DF)
    }
    return(outDF)
}
performanceBuild <- function(buildDF, timeHorizon, perfInit){
    allDF <- data.frame()
    fileExport <- paste0("performance",
                                format(Sys.time(), "%Y-%m-%d-%I-%M"),
                                ".csv")
    for (s in 1:max(buildDF$scenario)){
        workDF <- filter(buildDF, scenario == s)
        p <- perfInit
        startTime <- 0
        scenDF <- data.frame()
        for(r in 1:nrow(workDF)){
            timeCol <- seq(from = startTime,
                           to = startTime + workDF$time[r] - 1,
                           by = 1)
            perfLevel <- rep(p, workDF$time[r])
            eventDF <- data.frame(Time = timeCol,
                                  Performance = perfLevel,
                                  Scenario = s)
            startTime <- startTime + workDF$time[r]
            p <- workDF$Level[r]
            scenDF <- bind_rows(scenDF, eventDF)
        }
        allDF <- bind_rows(allDF, scenDF)
    }
    write.csv(allDF, file = fileExport)
    return(fileExport)
}

g <- scenarioBuild(1/10, 100, 3)
currentFile <- performanceBuild(g, 100, 1)
print(head(g))
h <- read.csv(currentFile)
print(head(h, 50))
print(dim(h))
```
## Build Stakeholder Preferences

My next step is to build it soe the integral resilience metric is
applied. This is non-trivial. I will also need to generate stakeholder
preferences from probabilistic and possibilistic methods and append
them to the performance profile before I can calculate resilience. 
Let's build a framework that can take the stakeholder preference and
assign random steps? a linear trajectory? I'm not sure. OK, why not
both?
First, the simplest - assume constant need, but it may be distributed
triangularly.

# How the performance preference profiles will be generated.
Build a list of the parameters I want it to have. Looking at the
inputs to the current resilience for the infrastructure problem:
multScenarioFast 
fileNames
Need list
Resilience list
Time Horizon
Calls:
cleanAnylogic - list of filenames
multInfrastructureFast - all inputs provided by multScenarioFast
 

## Build constant stakeholder need profiles
Here is an attempt to split up multiple performance and preference
profiles. In order to incorporate a time-dependent sigma we will
need a column of those as well.

```{r Preference Building}

library("triangle")
minTH <- 85
maxTH <- 115
modeTH <- 105
numScenTH <- 5
timeHorList <- ceiling(
    rtriangle(n = numScenTH,
              a = minTH,
              b = maxTH,
              c = modeTH))
minNeed <- .7
maxNeed <- 1.05
modeNeed <- .9
numScenN <- 10
cnstNeedList <- rtriangle(n = numScenN,
                          a = minNeed,
                          b = maxNeed,
                          c = modeNeed)

preferenceParams <- expand.grid(TimeHorizon = timeHorList,
                                 NeedLevel = cnstNeedList)


```
   
# Extended Integral Resilience
Build extended integral resilience value from what we have. 
1. Split the data.frame up by perf > Need
2. for Need > perf -> trapz perf / trapz need
3. for per > need -> 1 + chi(trapz perf / trapz need
4. R_EIR = sum of 1 & 2 / T_h

```{r Extended Integral Resilience}
assignGroup <- function(DF){
    i <- sign(DF$diff[1])
    grp <- 1
    DF$Grp <- 1
    for (r in 2:nrow(DF)){
        if (sign(i) == sign(DF$diff[r])){
            DF$Grp[r] <- grp
        } else {
            grp <- grp + 1
            i <- sign(DF$diff[r])
            DF$Grp[r] <- grp
        }
    }
}

perfPrefDF <- function(performanceDF, preferDF){
    for (s in 1:max(performanceDF$Scenario)){
        workDF <- filter(performanceDF$Scenario == s)
    }
}

singleResilience <- function(perf, stake){
    
}

```
### NHPP
Next is the Nonhomogeneous Poisson process (NHPP)

### Renewal process
Finally is the general case of a renewal process.
## Robustness
Robustness can vary from the previous value to zero, so it must be a
bounded distribution of some type. Beta is probably a good
distribution for this one.

## Time to recover
Time to recover is similar to time to repair. In the literature (SHOW
THIS) a lognormal distribution can be a good value for it.
## Level of recovery
Level of recovery is like robustness, so again, a bounded equation,
but recovery may be greater than the previous value, so the upperbound
may be a distribution itself. Try to bound within reality.
## Stakeholder time horizon
Defined by the stakeholder.
## Stakeholder endogenous preference
Look into the prognostic and predictive work. how do economists
predict future demand?
## Stakeholder intertemporal substitutability
Look more seriously at the economic description of it and make it
apply to what we are doing today.
